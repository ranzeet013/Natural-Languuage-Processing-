{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee7554f3",
   "metadata": {},
   "source": [
    "### Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aaed360",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Tokenization is the process of breaking down the given text in natural language processing into the smallest unit in a sentence called a token. Punctuation marks, words, and numbers can be considered tokens.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d337d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization is the process of breaking down the given text in natural language processing into the smallest unit in a sentence called a token. Punctuation marks, words, and numbers can be considered tokens.\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6eb223c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tokenization',\n",
       " 'is',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'breaking',\n",
       " 'down',\n",
       " 'the',\n",
       " 'given',\n",
       " 'text',\n",
       " 'in',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'into',\n",
       " 'the',\n",
       " 'smallest',\n",
       " 'unit',\n",
       " 'in',\n",
       " 'a',\n",
       " 'sentence',\n",
       " 'called',\n",
       " 'a',\n",
       " 'token.',\n",
       " 'Punctuation',\n",
       " 'marks,',\n",
       " 'words,',\n",
       " 'and',\n",
       " 'numbers',\n",
       " 'can',\n",
       " 'be',\n",
       " 'considered',\n",
       " 'tokens.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2112b489",
   "metadata": {},
   "source": [
    "### Sentence Tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6877ea8",
   "metadata": {},
   "source": [
    "Sentence tokenization is like cutting a big block of text into smaller pieces, where each piece is a sentence.\n",
    "\n",
    "Because breaking text into sentences helps computers understand and work with the text better. It's like breaking down a big problem into smaller, more manageable parts. This way, computers can analyze and process the text more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0ceabcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization is the process of breaking down the given text in natural language processing into the smallest unit in a sentence called a token.', 'Punctuation marks, words, and numbers can be considered tokens.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "sentence_tokens = sent_tokenize(text)\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7af7900a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization is the process of breaking down the given text in natural language processing into the smallest unit in a sentence called a token.', 'Punctuation marks, words, and numbers can be considered tokens.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "sentence_tokens = sent_tokenize(text)\n",
    "print(sentence_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baff9a47",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08c8458f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'down', 'the', 'given', 'text', 'in', 'natural', 'language', 'processing', 'into', 'the', 'smallest', 'unit', 'in', 'a', 'sentence', 'called', 'a', 'token', '.', 'Punctuation', 'marks', ',', 'words', ',', 'and', 'numbers', 'can', 'be', 'considered', 'tokens', '.']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_tokenize(text)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c83db3e",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadb854a",
   "metadata": {},
   "source": [
    "Lemmatization is the process of reducing words to their base or root form, known as the lemma. Unlike stemming, which simply chops off the ends of words, lemmatization considers the meaning of the word and converts it into its base form, which is linguistically valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0d8609d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization is the process of breaking down the given text in natural language processing into the smallest unit in a sentence called a token . Punctuation mark , word , and number can be considered token .\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "words = word_tokenize(text)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "lemmatized_text = ' '.join(lemmatized_words)\n",
    "\n",
    "print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31a5bd5",
   "metadata": {},
   "source": [
    "### Part-of-speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb58b49",
   "metadata": {},
   "source": [
    "Part-of-speech (POS) tagging is the process of assigning a grammatical category (such as noun, verb, adjective, etc.) to each word in a sentence based on its syntactic role. It's an essential task in natural language processing (NLP) for understanding the structure and meaning of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e09277d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Tokenization', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('process', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('breaking', 'VBG'),\n",
       " ('down', 'RP'),\n",
       " ('the', 'DT'),\n",
       " ('given', 'VBN'),\n",
       " ('text', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('natural', 'JJ'),\n",
       " ('language', 'NN'),\n",
       " ('processing', 'NN'),\n",
       " ('into', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('smallest', 'JJS'),\n",
       " ('unit', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('sentence', 'NN'),\n",
       " ('called', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('token', 'NN'),\n",
       " ('.', '.'),\n",
       " ('Punctuation', 'NN'),\n",
       " ('marks', 'NNS'),\n",
       " (',', ','),\n",
       " ('words', 'NNS'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('numbers', 'NNS'),\n",
       " ('can', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('considered', 'VBN'),\n",
       " ('tokens', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "pos_tag(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3855efd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
